# The RoboCEO

This podcast was part of the [Chaos Orchestra](https://www.youtube.com/@chaosorchestra8927) series of podcasts hosted by [Boris Shalumov](https://www.linkedin.com/in/boris-shalumov-86b5ba100/).

[YouTube Recording of Podcast](https://www.youtube.com/watch?v=b0Qp5TooW9A)

**Recording Date:** Apr 22, 2021

## Summary

In this episode of the Chaos Orchestra podcast, Dan McCreary explores the concept of the "Robot CEO," where AI systems, particularly knowledge graphs, integrate with human decision-making to enhance business operations. Dan discusses the current limitations and future potential of AI in enterprise settings, predicting the increasing adoption of knowledge graphs, the standardization of graph query languages, and the integration of machine learning to drive innovation and scalability. He also addresses the cultural and strategic shifts necessary for companies to stay competitive in a rapidly evolving technological landscape, emphasizing the transformative power of connected data and AI-assisted decision-making.

## Sections

Here are eight chapter headings for the podcast:

1.  [Introduction to the Robot CEO Concept](#introduction-to-the-robot-ceo)
2.  [Modeling and Chaos](#modeling-and-chaos)
3.  [Better Decisions](#better-decisions)
4.  [The Value of Collective Intelligence](#the-value-of-collective-intelligence)
5.  [Symbiosis of the CEO and AI](#symbiosis-of-the-ceo-and-ai)
6.  [Explainability](#explainability)
7.  [Combining Machine Learning and Knowledge Graphs](#combining-machine-learning-and-knowledge-graphs)
8. [Fear of Adopting New Technologies](#fear-of-adopting-new-technologies)
8. [Predicting Time to the Intelligent Organization](#predicting-time-to-the-intelligent-organization)


[Music]

## Introduction to the Robot CEO

Host: Welcome to Chaos Orchestra, Episode 1: The Robot CEO.

The idea of the Robot CEO is that AI does not necessarily need to mimic human intelligence in order to provide huge value. Instead, we could seek a symbiosis with AI, using unique machine capabilities to complement complex reasoning and decision-making, which currently is only accessible to humans. The Robot CEO would be a symbiotic system of a human and a machine, featuring a CEO or any executive who can interact with an AI system storing a vast index of dynamic information. This system would allow them to both gain insights and decide courses of action.

Now, let's look at how decisions are made in big companies. Usually, everything happening in a company or relevant to a particular decision is abstracted to a dozen KPIs presented to a CEO. Then, they have to make a decision that will impact a significant part of the company. It's nearly impossible to predict all the consequences and the impact this decision will have. The question is, how many executives are actually making these decisions more or less blindly, without knowing the full reach of the consequences?

AI systems, in theory, have the ability to solve this problem by processing this huge amount of information and finding the optimal path for a particular decision. In actual fact, these systems are usually not so robust and need a vast amount of information, data, and scenarios to be able to give a proper recommendation. Even if they could do all this, would an executive—constantly watched and evaluated by the public, media, employees, and stakeholders—rely on the recommendation of an AI system that gives nothing more than a percentage of certainty as rationale?

One might argue that executives trust their gut to account for all the information and data they do not have, but this is a slightly different topic. Nowadays, AI system decisions are judged far more strictly than those made by humans. So what if an executive could actually work together with a vast, intelligent, continuously updating knowledge base? What if the system could communicate, give recommendations, and calculate changing circumstances, all based on instructions from a human partner?

Perhaps it would be enough for an executive to know the logic of the AI recommender system—how it was trained, what conditions it operates best under, what factors it does not include, and what algorithms it's based on—in order to understand the limitations of a particular system. Imagine being able to leverage a machine as a tool to understand in what context the AI system can produce what results and what additional information you might need to gather to increase the significance of these results.

So, we do not need machines to mimic human intelligence to achieve superior results. Machines do not yet have the level of reasoning that humans have, and people cannot account for the volume of facts that are relevant for complex decisions.

Today, we're going to talk about how a system like that could look and what opportunities and challenges it will bring. Our guest today is Dan McCreary, a lead thinker and one of the most inspiring people in the knowledge graph space. He is also the author of the article "The God Graph." Dan, can you explain what the God Graph actually is?

**Dan McCreary:** What was really inspiring about this concept is my background in physics and my interest in following the developments at CERN. There was a journalist who called the Higgs-Boson the "God particle," and it became a very popular meme. The idea behind it was to find one particle that could create a unifying principle, tying a lot of concepts together. If they could discover this particle, they could eliminate all the other theories and have a unifying force.

That's what I was trying to do with the term "God Graph" — to bring together a lot of ideas around knowledge graphs and systems thinking. I have several friends who are really good systems thinkers, and I've always thought of it as a weakness of mine that I didn't study systems thinking much. What's important about this too is the focus on ethical considerations and what we call externalities.

Every day, I'm involved in making decisions about how to grow our internal enterprise knowledge graph. I like to visualize this as an amoeba—an amorphous blob—where inside the cell wall are the things you control, the things you model, and the things in your company that you have a pretty good sense of. As enterprise knowledge graphs grow, that area gets bigger. The edge of the cell wall is what I call the "edge of chaos."

The edge of chaos is a highly loaded term, so we should probably define it carefully. What's important is that outside your knowledge graph are the things you have not modeled, which we often call the chaotic world. These are the things you don't understand yet, don't have a good model for, and can't make predictions about. They are outside your knowledge graph.

What we try to do is make a decision every week in our committee meetings about how we should expand our knowledge graph. Right now, we do a cost-benefit analysis and try to predict whether that additional data will add value to our company. But your question was really about systems thinking, externalities, and the social consequences of our decisions. What about the environment, social justice, and poverty, among other things?

To be honest, we think about these things a little, and we have social determinants of health metrics that look at a lot of these factors. We focus on interventions in suicide prevention and regional opioid addiction. But we're not really modeling the outside world much, and only a small percentage of the people I talk to have an idea of what we could do if we modeled that.

The God Graph was this idea of what would happen if we started to model and simulate the consequences of our decisions on the outside world. A good example is companies moving to sustainable energy—like Apple and Google, building huge solar plants to power their data centers and doing carbon offset initiatives. They know they want their brand to be associated with sustainability. Even though it costs them more, they believe they are making the right decisions in the long term because of the social consequences.

That's the type of thinking I want our organizational strategists to take into account.

## Modeling and Chaos

**Host:** You've described the consequences and the impact that we cannot calculate or predict because of this chaos that is not modeled outside our company. Aren't some companies too confident in their models within their company? There could still be the same butterfly effect within the company. Maybe we're just not able to see it, but there's information or knowledge passed from an employee to their manager and then from that manager to another, abstracted over the hierarchies and usually reduced to some KPIs. These KPIs often don't consider every detail, and this is where we lose information. What do you think about the chaos within and the butterfly effect within an enterprise?

**Dan McCreary:** That's a very good point. A lot of this has to do with randomness and uncertainty in our environments. You mentioned that we are overconfident in our models, and you're absolutely right. COVID-19 is a really good example of how we had all these healthcare models, and they completely collapsed when COVID emerged because the rules were different.

There are certain events that we can predict with some probability, but they are very unlikely events. Hurricanes, for example, can be predicted, but we can't tell you exactly when or where they will land. This is also true in financial markets, where you're trying to predict which stocks to invest in.

When you think about technology, technology has all these butterfly effects. Many people can look at Moore's Law and say, "Yes, we can predict with relative certainty that processing power will increase, that memory will grow in capacity, and that the cost of storage will go down." Those are easy predictions because hundreds of thousands of engineers are working to lower the cost of that infrastructure every day.

What we cannot predict are breakthroughs and insights in certain innovations. One of the best examples is AlexNet. You've probably heard the stories of AlexNet, where they had this competition to recognize objects in images every year. They were making one or two percent increases each year, and then all of a sudden, one guy figured out how to use a GPU to train his models, resulting in a 10% improvement in the scores. Then everybody used that same model, leading to dramatic increases.

So, you don't get a linear growth in innovation because of these ideas. The butterfly effect makes it difficult to predict the impact of innovation. Most of this innovation happens in software. While hardware continues to improve, we cannot predict insights in algorithms, machine learning, and AI, especially in NLP innovations like BERT and GPT-3. I've been using GPT-3 to create outlines for my blogs, and it's just phenomenal.

What's interesting is to think through the general strategic direction of these innovations and realize that they affect how CIOs and CEOs do their strategy. Strategy is effectively a statement of your organizational goals—where you want to go. The RoboCEO's job is not just to set that strategy but to monitor all the tasks we can do to reallocate resources so we can move in that direction.

This is where the enterprise knowledge graph can provide feedback. A lot of systems thinking is about feedback. Many CEOs write a huge document that says, "Here's our strategy for the next year," and then they throw it over the wall and say, "Good luck." But they can't monitor what's really going on at the fine-grained level. That's where I think the relationship between the RoboCEO and the knowledge graph comes into play—to start getting that feedback and using it to constantly change your strategy to ensure your organization is going in the right direction.

## Better Decisions

**Host:** CEOs make decisions every day, sometimes dozens of decisions that impact thousands, or even more, people inside and outside the enterprise. How are CEOs making these decisions now, and how can enterprise knowledge graphs and other technologies help them make better decisions or at least be more aware of the consequences and impact?

**Dan McCreary:** Great question. I would say I’m not the world’s leading thinker on what I call "strategy modeling," but I have friends who are some of the world’s leading thinkers in this area. Much of this goes back to work done by people in U.S. federal agencies who were trying to research how to make a federal agency’s strategy more transparent to stakeholders. A group of people said, "Well, what we first need to do is create a model of what that strategy is." A strategy breaks down into a set of sub-components, objectives, and things like that. It’s often written in a way that’s a document, but it’s not easily decomposable into a graph.

We can start to change that by using a representation called StratML, the Strategy Markup Language. What’s interesting about StratML is that it allows us to break down a strategy into a series of initiatives, which are related to concepts. Those concepts can be identified at the fine-grained level of everything an enterprise does. Every time you send an email, we could analyze that email and look at the concepts discussed.

If a CEO says, "We need to have better cybersecurity," and that’s one of our initiatives for the next year, they could create a dashboard that shows how many people are talking about cybersecurity issues. Here’s the tragedy: many times, after the CEO announces these things, you see a spike in interest, discussions, and meetings, but then it drops off.

What an enterprise knowledge graph can do is provide ongoing feedback. When I think of the knowledge that flows in and out of an organization—and when I use the word "organization," I include every federal agency, state agency, and NGO; they are all organisms that interact and exchange information—I think of three things. One is data: how much data do we have? The second is semantics: do we understand that data, the meaning behind it? And the third is trust: how do we know that the data is valid, correct, and uncorrupted?

There have been discussions about using technologies like blockchain to verify that knowledge is coming from a trustworthy source. For example, in the open linked data movement, publishing linked data as RDF allows it to be linked precisely, which reduces the huge costs of exchanging this knowledge. Data.gov was a big movement for a long time, aimed at getting federal agencies not just to throw out a spreadsheet of their data but to publish it in a way that we could ingest into our knowledge graphs and have it be meaningful and impactful for our predictions.

We go back and forth on this, but with the new generation of hardware coming out, it’s going to be absolutely incredible. There will be even more demand for precise, clean data, and there will be an opportunity for entrepreneurs to normalize and make this data consistent in a linked format, and then provide it as a service. We can then find vendors we trust, who have been doing the cleanup and verifying the veracity and consistency of this data.

## The Value of Collective Intelligence

**Host:** We have a lot of data within companies and enterprises, and now we have the technology to access, process, and evaluate this data, whether it’s structured or unstructured. We have NLP, computer vision, and a huge amount of power to access and evaluate results. But with great power comes great responsibility. There’s also the potential for misuse, as we've seen with social networks where the same technologies used to spread ideas can also spread misinformation.

If we bring this back to the enterprise context, assuming we have a knowledge graph that ties together all models in the enterprise and we can process all the thoughts, ideas, and knowledge of individual employees—something we might call "collective intelligence"—how valuable might that be in a big company? How could it work, and how do you quantify its importance or value?

**Dan McCreary:** Great question. I constantly go back to biological metaphors, so let’s explore that a bit. One of the things about our brain is that we have higher-order areas that motivate us to do certain things. If we haven’t eaten for a long time, we feel hunger, which changes and reprioritizes all our actions. What’s interesting is that most knowledge graph systems I work in do not have a subgraph for the company’s strategy or goals.

So, let’s assume that some of the work done on StratML does get converted so that an organization could have a high-level strategy. Remember, many organizations have multiple business units, each with its own strategy. Sometimes these strategies align, and sometimes they don’t. Many companies have decentralized control models where each business unit is responsible for its prosperity—take Berkshire Hathaway, for example, which buys companies but doesn’t integrate them; they’re just financial assets.

Other organizations have much better centralized control, where they have a big organizational strategy, hold strategy meetings, and everyone brainstorms and aligns their actions to that strategy. What if a subgraph in our knowledge graph represented those strategies? What if we could align all activities with those strategies and provide feedback and reward systems accordingly?

Remember, our brains have constant feedback loops. If we’re hungry and we eat, we feel satiated and no longer hungry. We don’t have those feedback systems in organizations. So, the first thing we need to do to build this RoboCEO is to model our organizational goals and strategies. The second thing is to have sensors—just like our fingers, ears, and eyes—to sense what’s going on in our organization. We don’t need raw data; we need to create information from that data, classify it, and then provide feedback systems.

For instance, if you want to go in a certain direction, but we see that strategy is not working or that a business unit has its own conflicting ideas, we need to realign things. I should mention that I work for a company with very decentralized control, so I see these issues all the time. Sometimes it feels like an army marching across a continent with everyone walking in random directions.

The RoboCEO is something we desperately need to align many internal company functions, but we also need to ensure it aligns with our social responsibility goals. However, it will take time to get there.

## Symbiosis of the CEO and AI

**Host:** The idea of the Robot CEO is not just to use AI to replace the CEO, but to create a symbiosis where the human CEO is supported by an AI system. The AI could process huge amounts of information, both from within and outside the company, and handle unstructured data. So, Dan, what do you think? How might this symbiosis work, what challenges could arise, and how could it be implemented?

**Dan McCreary:** There’s a lot to consider here. When we think about the scalability of a RoboCEO, it’s important to note that human beings have limited scalability. We can’t monitor every log feed of every operation and every code check-in. The CEO needs a cognitive assistant they can have a dialogue with, to get feedback.

The idea is not just one but a series of cognitive assistants that populate an executive’s dashboard. Whenever we do a knowledge graph project, we perform a cost-benefit analysis. Costs are easy to measure: servers, software, people to load the data. Productivity of knowledge workers can also be measured: if a call center has to log into 15 different systems, the average phone call might take an hour. But if they have all the knowledge in front of them and a single view of the customer that displays in 100 milliseconds, those calls could take just five minutes, and most importantly, customer satisfaction would dramatically increase.

What’s hard to measure is the idea of the butterfly effect—the impact of innovation, the insights that could save us millions of dollars. Recently, one report was projected to save our company $100 million. Could we predict that? No, but I can tell you that if our data scientists had to extract subsets of data from 15 different systems, normalize it, and make it consistent, it would have taken six months and cost two to three million dollars. But since the data was in the knowledge graph, we were able to generate 130 series of reports that honed in on the right data, working with our clinicians and physicians who suspected fraud or waste.

It’s a discovery process, and I like to call that "strategic serendipity." Just like the name "Chaos Orchestra," which combines the ideas of uncontrolled chaos and orchestrated order, strategies to support serendipitous discoveries require holding oppositional concepts in mind. We’re creating a platform that allows for innovation, even though we cannot predict it.

## Explainability

**Host:** We’ve made huge advancements in AI in the past decades, but there’s still a problem with explainability. AI recommendations produced by neural networks often come from a "black box" that takes input and gives results without clear reasoning. This is a challenge for CEOs who have to make strategic decisions, like restructuring a company or deciding where to build a new plant. These decisions have a huge impact on the company’s future. What would it take for a CEO to trust a system like this, and what’s the role of knowledge graphs in building that trust?

**Dan McCreary:** Great question. Knowledge graph architecture is a core piece of the explainability puzzle. To visualize this, consider a deep learning neural network like GPT-3, which has 175 billion parameters. Text comes in, it goes through the network, and generates output. But if you ask how or why it generated that output, it’s very difficult to explain.

Deep learning neural networks alone have almost zero explainability. Knowledge graphs, on the other hand, often include a subgraph known as a semantic concept graph. This subgraph contains concepts that have ideally been curated by human beings. In healthcare, for example, we have about 3.4 million concepts in our knowledge graph—every drug, symptom, disease, procedure, billing code, and definition related to a patient and their condition.

So, when you traverse that concept graph, you accumulate a series of definitions as you move through it, and you can see the path that was taken. That path provides a basis for explainability. For example, if a patient requests a very expensive drug or procedure, we have something called "prior authorization." A good example is infertility treatment, such as in vitro fertilization (IVF), which can cost around $20,000. We want to ensure that the patient has done all the necessary due diligence before authorizing such an expensive procedure. We want to make sure they've been seeing an infertility specialist for at least a year before moving forward.

If we authorize the treatment, there's no need for an explanation—people are generally satisfied with that outcome. But when we deny the treatment, we have to provide a legally binding explanation, reviewed by our attorneys, to justify why the request was denied. This process is very time-consuming and expensive, which makes explainability in healthcare incredibly important.

No physician in their right mind would blindly apply a series of AI-generated recommendations to a patient without understanding the rationale behind them. Clinical decision support systems absolutely need explainability. We're gradually working toward a process where enterprise knowledge graphs incorporate this concept of explainability.

The way I like to think about it is that if you have a three-stage process—input data, deep neural network processing, and output—there’s no explainability in that system. But if you insert a knowledge graph between the deep neural network and the output, the machine learning system builds concepts, relationships, and causal diagrams within the knowledge graph, which can then be explained. This knowledge graph acts as an interface between the deep neural networks and the explainable interface, and it is a crucial piece of the puzzle that we are working on.

In this architecture, the knowledge graph allows us to traverse it and create a written explanation based on the data. Sometimes these explanations need to include a link to the research paper showing that a particular drug is ineffective or effective in a specific combination. Often, in cases of off-label drug use, there may be no data to prove efficacy, which can be very controversial. Research is always evolving, and sometimes a physician may have access to research that our internal teams do not, which they then submit for review. We have to digest that research, assess its validity, and potentially change our policy based on the new information.

This is a continual process, and we spend millions of dollars every year trying to explain decisions in healthcare. AI is only getting slightly better at explaining these decisions—not nearly as fast as we’d like, but we’re making progress. Knowledge graphs are central to building this explainability process.

## Combining Machine Learning and Knowledge Graphs

**Host:** So, to achieve explainability, you basically need a combination of symbolic AI and statistical AI, creating a system that aggregates knowledge from across the enterprise. It’s a blend of different approaches that allow you to gather voices from across the organization, similar to a social network, and integrate them into decision-making. But let me play the devil’s advocate here—how do you bring in the knowledge that resides in the heads of experts who may have more information than any database? These are people who have been with the company for decades. How do you make all this information accessible for the CEO and the RoboCEO to use in making informed decisions?

**Dan McCreary:** One metaphor I often use is that of Wikipedia. Why do we trust Wikipedia? The answer is that a lot of people are constantly watching Wikipedia to ensure that the knowledge it contains is trustworthy. When Wikipedia first started, I was very involved, contributing around 15,000 edits over the first few years to understand how it works.

What’s important about Wikipedia is the continual review process, where social networks of contributors work together. For every 9,999 people trying to improve Wikipedia, there’s usually one person trying to disrupt it or add misinformation. Because of the overwhelming majority of good actors, we have a relatively high probability that Wikipedia’s content is accurate. This doesn’t mean every page is correct all the time, but it does mean there’s a strong likelihood that bad information will be corrected quickly.

We have similar processes within healthcare. Research articles are peer-reviewed, and the peer review process helps ensure that the science behind these articles is sound. We trust certain publications and research institutions because of their rigorous review processes. However, there are also websites promoting pseudoscience or unverified treatments, like certain aspects of the vitamin industry, which may not have scientific support.

Within an enterprise, the challenge is to continually build better knowledge graphs that can incorporate and validate this expert knowledge, ensuring that it can be trusted for decision-making. It’s a balancing act between ensuring that the information is accessible and trustworthy while also protecting it from misuse.

## Fear of Adopting New Technologies

**Host:** Human beings are complex, and fear is a fundamental emotion that can prevent the adoption of new technologies, especially when it comes to something as potentially threatening as knowledge graphs. There’s a cultural aspect to this—some people might fear that making everything transparent and accessible might diminish their importance or change their role. How do you address this fear and ensure that knowledge graphs are adopted within a corporate culture?

**Dan McCreary:** That’s a crucial point. Corporate culture plays a huge role in the adoption of new technologies. In the Bay Area, when you work with companies like Apple, Google, Amazon, Facebook, and Twitter, you see that their entire existence is predicated on the ability to scale. Everything they do—their culture, hiring, onboarding, and training—focuses on scalability.

At Amazon, for example, it’s a fireable offense to put any data in a relational database that has to scale. The reason is that they’ve had so many experiences where older relational databases, designed to bring everything into memory and then compare data in two columns to resolve a relationship, just don’t work at scale. If you have a small accounting system keeping track of a general ledger, that’s fine; you get an exception. But when it comes to scaling, those old systems can’t keep up.

The company culture at these tech giants is that scalability is a core value. But many other companies don’t have that same focus. They may prioritize short-term financial goals, aiming to meet quarterly targets so executives can get their bonuses. These companies are never going to make the leap from the old world of COBOL and mainframes to highly scalable enterprise knowledge graphs because they lack the core values that emphasize scalability and shared knowledge.

So, who’s responsible for setting this culture? It’s the CEO who must take the lead in establishing a culture that values scalability and the responsible use of shared knowledge. Companies like Apple make decisions based on the expertise of subject matter experts, not accountants. They let the experts in technology make the technology decisions, which is why they’ve been so successful.

In contrast, companies driven by short-term thinking, financial targets, and cost-cutting measures are less likely to succeed in the long term. They lack the same ability to innovate and scale as companies like Apple, Google, and Facebook because their corporate culture doesn’t support it. This is why the economy has seen such growth in these tech giants over the last decade—they’ve been able to scale because of their culture.

**Host:** And most of these successful companies have a knowledge graph behind the scenes, right?

**Dan McCreary:** Absolutely. That’s a very good point. It’s not just about having a knowledge graph; it’s about the scalability of those systems. Amazon, for example, has built their shopping carts on key-value stores, which are designed for scalability. LinkedIn, Facebook, and of course, Google’s knowledge graph are all about not just one graph in one department but about enterprise-wide knowledge and its connectivity.

What’s interesting is that these knowledge graphs are often product-focused, but they also have an impact on internal operations and business. Companies that focus on graph models tend to have higher growth rates and profitability than those still relying on traditional ERP-based systems.

If you look at trend reports on graph databases, there’s a company called DB-Engines that tracks the popularity of different database technologies. Graph databases are growing in interest faster than any other type, but adoption doesn’t always keep pace with interest. Interest is a leading indicator, but what I will say is that because of this growing interest, hardware companies have started to take notice.

For example, Graphcore, a company out of the UK, has built an impressive 50 billion transistor chip optimized for graph analytics, and they recently secured $200 million in additional funding. Parallel processing technologies like FPGAs are also being customized for graph workloads. Another mind-blowing development was the Intel PUMA project, a next-generation chip focused solely on graph traversal.

I’ve always had an intuition as a hardware guy—you can see the chips on the wall behind me that I’ve designed in the past—that pointer hopping through a graph only uses about 10% of traditional instructions. By eliminating the unused instructions, you could get 10 times more cores, which is obvious to anyone who knows instruction sets. What I didn’t fully appreciate was how inefficient memory management is in traditional graph processing. DARPA and Intel’s research into this led them to project a 1,000-fold increase in performance with the new generation of silicon. That’s three orders of magnitude better performance, or potentially one-thousandth the cost of today’s hardware.

What this means is that we’re now thinking in systems terms at the hardware level for graphs, which we’ve never done before because graphs weren’t a big piece of the market share. But now, as graph adoption picks up, we’re seeing very large implementations, and the cost-effectiveness of graph hardware is going to start playing a significant role.

The question then becomes, what’s going to happen to companies that don’t adopt these technologies? My answer is that there will be continued pressure on them to create divisions within their companies that focus on creating holistic views. And maybe "graph" is too specific a term—I prefer "connected data." There are many ways to achieve connected data sets; you don’t necessarily have to use a graph database. The point is that a company operating with a thousand isolated silos of data won’t be able to find the deep insights that a company with connected data can. Companies that bring all their data together into one system—whether it’s a graph database or something else—will have a significant advantage.

What concerns me is that this will likely concentrate even more power in a small number of companies that have absolute control over our world. Companies like Amazon, Google, and Facebook already wield enormous influence because of their ability to scale and integrate data. Amazon, in particular, is in a prime position with their graph projects designed to be easy to deploy on their cloud. However, these projects aren’t yet enterprise solutions—they’re more focused on consumer-facing applications.

Google, while using knowledge graphs internally, hasn’t yet offered a commercial graph product. I believe they’re waiting for the GQL (Graph Query Language) standards to mature. Once these standards are established, we’ll likely see an avalanche of companies offering graph-based solutions. The portability of code across different graph databases will be a game-changer, similar to what SQL did for relational databases. This portability will enable businesses to switch between different graph database vendors without being locked into one, fostering competition and innovation.

With better hardware and a standardized query language, we’ll see a surge in third-party solutions built on graph technology. Some companies will transition from traditional ERP systems to graph-based systems, possibly without users even noticing. These transitions could happen seamlessly, but the key is that companies need to be prepared for this shift.

We should be cautious about allowing too much control over large knowledge graphs to fall into the hands of a few companies. While Amazon is currently leading in graph-based cloud solutions, there’s still room for innovation. I expect the release of GQL and the continued development of graph-specific hardware to open up the market to a broader range of competitors.

## Predicting Time to the Intelligent Organization

**Host:** How far away are we from having a Robot CEO that can see through the space and time of an enterprise, aggregate all this knowledge, and make intelligent decisions while being fully aware of the consequences?

**Dan McCreary:** There’s a famous quote by Paul Saffo: "Never confuse a clear view with a short distance." It means that just because you can see the direction the industry is going doesn’t mean the journey will be quick or straightforward.

I always resist giving exact timelines because innovation is unpredictable. For instance, no one could have predicted the breakthroughs with AlexNet, BERT, or GPT-3. What I can tell you is that knowledge graphs are getting to a point where we can expect some incredible innovations in the next few years.

A lot of the discussions we’re having about the RoboCEO and the integration of strategy into our knowledge graphs will be central to these innovations. The trend is clear: graphs are going to get bigger, faster, easier to use, and more specialized. They will continue to grow in complexity and specialization, with subgraphs focusing on different aspects of the enterprise, such as security, customer data, operations, and more.

By next year, I expect at least a dozen companies will be offering solutions based on a standardized Labeled Property Graph (LPG) query language like GQL. This will allow for more competition and innovation in the graph database space. We’ll also see software packages that integrate machine learning directly with graph databases. For example, Neo4j’s graph machine learning library is a fantastic example of this integration, allowing data scientists to seamlessly build embeddings for every vertex in the graph.

By next year, any player in the graph space will need to integrate machine learning, where every vertex has an embedding. This will enable predictive analytics, recommendations, and other advanced capabilities. Companies that continue to rely on scalable relational databases will likely hit a wall because they won’t be able to leverage the new hardware, query languages, and graph machine learning capabilities.

Looking three years out, we’ll see the synergistic relationship between scalable graphs, highly specialized subgraphs, and natural language processing. This will give rise to a new generation of vendors offering integrated, cloud-based turnkey solutions for various industries. Imagine a turnkey ERP system that’s knowledge graph-based. You could start a new company, open a cloud account, load some data, get customers, harvest data, find insights, and offer services—all within a graph-based system.

In three to four years, we’ll see the maturation of machine-readable geospatial reasoning, natural language processing, and policy analysis. These capabilities will enable companies to make complex decisions, like where to build a new plant, by leveraging machine-readable incentives, regulations, and other factors. This will mark a significant shift in how businesses operate and make decisions.

Beyond that, looking five years into the future, it’s challenging to predict. Many people ask when enterprise knowledge graphs will become self-aware, like Skynet. In about ten years, we might see knowledge graphs capable of doing their own planning, predicting tasks, and assigning resources and people accordingly. This would involve tearing down rigid organizational structures and creating a more dynamic, skills-based allocation of resources.

As we move toward this future, knowledge graphs will play a crucial role in predicting which skills employees need to be most effective, which projects they should work on, and how to mentor and develop talent within an organization. The end goal is to create a seamless integration between human intelligence and AI, where the AI understands and assists with complex decision-making, ultimately leading to a more efficient and innovative enterprise.

That’s my long-term prediction—knowledge graphs will continue to grow in complexity, specialization, and integration with AI, leading to a future where the RoboCEO is not just a concept but a reality.











ChatGP